{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d5da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1688f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data1.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96112731",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff6520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bcdaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e572c0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in q:\n",
    "        q.append(i)\n",
    "        \n",
    "data = ' '.join(q)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16f6d7",
   "metadata": {},
   "source": [
    "# Tockenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7250a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8, 731, 19, 732, 9, 295]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10be2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79921bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab4048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1058205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4796a0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ...,   0,  53, 293])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(X) for x in sequences])\n",
    "input_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816114b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7f5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae76bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 100)            261700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              264317    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 696,917\n",
      "Trainable params: 696,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb68dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a6633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62faf5c8",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "438c84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6527db6",
   "metadata": {},
   "source": [
    "# Compile The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca16d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ed6d5",
   "metadata": {},
   "source": [
    "# Fitting The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1ca193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 7.8743 - accuracy: 7.9449e-04\n",
      "Epoch 1: loss improved from inf to 7.87442, saving model to nextword1.h5\n",
      "61/61 [==============================] - 5s 17ms/step - loss: 7.8744 - accuracy: 7.7141e-04 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 7.8650 - accuracy: 0.0026\n",
      "Epoch 2: loss improved from 7.87442 to 7.86500, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.8650 - accuracy: 0.0026 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 7.8586 - accuracy: 0.0039\n",
      "Epoch 3: loss improved from 7.86500 to 7.85844, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.8584 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 7.7446 - accuracy: 0.0027\n",
      "Epoch 4: loss improved from 7.85844 to 7.74679, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.7468 - accuracy: 0.0026 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 7.4945 - accuracy: 0.0037\n",
      "Epoch 5: loss improved from 7.74679 to 7.49694, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.4969 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 7.2814 - accuracy: 0.0036\n",
      "Epoch 6: loss improved from 7.49694 to 7.28396, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.2840 - accuracy: 0.0036 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 7.1767 - accuracy: 0.0045\n",
      "Epoch 7: loss improved from 7.28396 to 7.17891, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 7.1789 - accuracy: 0.0044 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 7.0940 - accuracy: 0.0023\n",
      "Epoch 8: loss improved from 7.17891 to 7.09578, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 7.0958 - accuracy: 0.0023 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0434 - accuracy: 0.0028\n",
      "Epoch 9: loss improved from 7.09578 to 7.04340, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 7.0434 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.9934 - accuracy: 0.0040\n",
      "Epoch 10: loss improved from 7.04340 to 6.99577, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 6.9958 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.9502 - accuracy: 0.0034\n",
      "Epoch 11: loss improved from 6.99577 to 6.95323, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 6.9532 - accuracy: 0.0033 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 6.9059 - accuracy: 0.0040\n",
      "Epoch 12: loss improved from 6.95323 to 6.91163, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 6.9116 - accuracy: 0.0041 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 6.8650 - accuracy: 0.0044\n",
      "Epoch 13: loss improved from 6.91163 to 6.86599, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 6.8660 - accuracy: 0.0044 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 6.8200 - accuracy: 0.0032\n",
      "Epoch 14: loss improved from 6.86599 to 6.82336, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 6.8234 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.7734 - accuracy: 0.0037\n",
      "Epoch 15: loss improved from 6.82336 to 6.77890, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 6.7789 - accuracy: 0.0036 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7269 - accuracy: 0.0049\n",
      "Epoch 16: loss improved from 6.77890 to 6.72689, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 6.7269 - accuracy: 0.0049 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.6512 - accuracy: 0.0048\n",
      "Epoch 17: loss improved from 6.72689 to 6.65286, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 6.6529 - accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.5924 - accuracy: 0.0041\n",
      "Epoch 18: loss improved from 6.65286 to 6.59241, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 6.5924 - accuracy: 0.0041 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.5154 - accuracy: 0.0058\n",
      "Epoch 19: loss improved from 6.59241 to 6.51425, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 6.5142 - accuracy: 0.0057 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 6.4353 - accuracy: 0.0070\n",
      "Epoch 20: loss improved from 6.51425 to 6.43652, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 6.4365 - accuracy: 0.0069 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 6.3620 - accuracy: 0.0068\n",
      "Epoch 21: loss improved from 6.43652 to 6.36474, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 6.3647 - accuracy: 0.0067 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 6.2737 - accuracy: 0.0077\n",
      "Epoch 22: loss improved from 6.36474 to 6.27665, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 6.2766 - accuracy: 0.0082 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 6.1686 - accuracy: 0.0081\n",
      "Epoch 23: loss improved from 6.27665 to 6.17082, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 6.1708 - accuracy: 0.0082 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 6.0175 - accuracy: 0.0075\n",
      "Epoch 24: loss improved from 6.17082 to 6.02320, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 6.0232 - accuracy: 0.0072 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 5.8218 - accuracy: 0.0100\n",
      "Epoch 25: loss improved from 6.02320 to 5.82046, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 5.8205 - accuracy: 0.0100 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 5.6203 - accuracy: 0.0119\n",
      "Epoch 26: loss improved from 5.82046 to 5.62669, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 5.6267 - accuracy: 0.0116 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 5.4532 - accuracy: 0.0151\n",
      "Epoch 27: loss improved from 5.62669 to 5.45856, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 5.4586 - accuracy: 0.0149 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 5.2978 - accuracy: 0.0169\n",
      "Epoch 28: loss improved from 5.45856 to 5.30022, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 5.3002 - accuracy: 0.0172 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 5.1695 - accuracy: 0.0177\n",
      "Epoch 29: loss improved from 5.30022 to 5.17393, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 5.1739 - accuracy: 0.0177 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 5.0513 - accuracy: 0.0198\n",
      "Epoch 30: loss improved from 5.17393 to 5.05216, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 5.0522 - accuracy: 0.0198 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.9360 - accuracy: 0.0244\n",
      "Epoch 31: loss improved from 5.05216 to 4.93603, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 4.9360 - accuracy: 0.0244 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.8330 - accuracy: 0.0249\n",
      "Epoch 32: loss improved from 4.93603 to 4.83304, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 4.8330 - accuracy: 0.0249 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.7512 - accuracy: 0.0249\n",
      "Epoch 33: loss improved from 4.83304 to 4.75118, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 4.7512 - accuracy: 0.0249 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.6614 - accuracy: 0.0296\n",
      "Epoch 34: loss improved from 4.75118 to 4.66137, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 29ms/step - loss: 4.6614 - accuracy: 0.0296 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 4.5757 - accuracy: 0.0318\n",
      "Epoch 35: loss improved from 4.66137 to 4.57578, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 4.5758 - accuracy: 0.0316 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.5009 - accuracy: 0.0347\n",
      "Epoch 36: loss improved from 4.57578 to 4.50091, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 4.5009 - accuracy: 0.0347 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.4392 - accuracy: 0.0332\n",
      "Epoch 37: loss improved from 4.50091 to 4.43918, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 4.4392 - accuracy: 0.0332 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.3751 - accuracy: 0.0357\n",
      "Epoch 38: loss improved from 4.43918 to 4.37511, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 4.3751 - accuracy: 0.0357 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 4.3146 - accuracy: 0.0424\n",
      "Epoch 39: loss improved from 4.37511 to 4.31772, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 4.3177 - accuracy: 0.0427 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.2617 - accuracy: 0.0388\n",
      "Epoch 40: loss improved from 4.31772 to 4.26169, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 4.2617 - accuracy: 0.0388 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 4.2074 - accuracy: 0.0466\n",
      "Epoch 41: loss improved from 4.26169 to 4.21063, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 4.2106 - accuracy: 0.0460 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.1360 - accuracy: 0.0447\n",
      "Epoch 42: loss improved from 4.21063 to 4.13597, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 4.1360 - accuracy: 0.0447 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.0842 - accuracy: 0.0566\n",
      "Epoch 43: loss improved from 4.13597 to 4.08417, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 4.0842 - accuracy: 0.0566 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 4.0306 - accuracy: 0.0547\n",
      "Epoch 44: loss improved from 4.08417 to 4.03412, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 4.0341 - accuracy: 0.0550 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.9878 - accuracy: 0.0620\n",
      "Epoch 45: loss improved from 4.03412 to 3.98776, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 3.9878 - accuracy: 0.0620 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.9419 - accuracy: 0.0579\n",
      "Epoch 46: loss improved from 3.98776 to 3.94191, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 3.9419 - accuracy: 0.0579 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8981 - accuracy: 0.0615\n",
      "Epoch 47: loss improved from 3.94191 to 3.89813, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 3.8981 - accuracy: 0.0615 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8514 - accuracy: 0.0651\n",
      "Epoch 48: loss improved from 3.89813 to 3.85141, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 3.8514 - accuracy: 0.0651 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8145 - accuracy: 0.0625\n",
      "Epoch 49: loss improved from 3.85141 to 3.81449, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 3.8145 - accuracy: 0.0625 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 3.7677 - accuracy: 0.0749\n",
      "Epoch 50: loss improved from 3.81449 to 3.77533, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.7753 - accuracy: 0.0738 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 3.7146 - accuracy: 0.0753\n",
      "Epoch 51: loss improved from 3.77533 to 3.71899, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 3.7190 - accuracy: 0.0743 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6851 - accuracy: 0.0771\n",
      "Epoch 52: loss improved from 3.71899 to 3.68513, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 28ms/step - loss: 3.6851 - accuracy: 0.0771 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 3.6495 - accuracy: 0.0852\n",
      "Epoch 53: loss improved from 3.68513 to 3.65411, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.6541 - accuracy: 0.0846 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6060 - accuracy: 0.0921\n",
      "Epoch 54: loss improved from 3.65411 to 3.60604, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 3.6060 - accuracy: 0.0921 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.5676 - accuracy: 0.0897\n",
      "Epoch 55: loss improved from 3.60604 to 3.56762, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 3.5676 - accuracy: 0.0897 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 3.5125 - accuracy: 0.0967\n",
      "Epoch 56: loss improved from 3.56762 to 3.52268, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.5227 - accuracy: 0.0946 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 3.4743 - accuracy: 0.1043\n",
      "Epoch 57: loss improved from 3.52268 to 3.48420, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.4842 - accuracy: 0.1036 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4561 - accuracy: 0.0982\n",
      "Epoch 58: loss improved from 3.48420 to 3.45608, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.4561 - accuracy: 0.0982 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4302 - accuracy: 0.1080\n",
      "Epoch 59: loss improved from 3.45608 to 3.43020, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.4302 - accuracy: 0.1080 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3890 - accuracy: 0.1085\n",
      "Epoch 60: loss improved from 3.43020 to 3.38903, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 3.3890 - accuracy: 0.1085 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3444 - accuracy: 0.1206\n",
      "Epoch 61: loss improved from 3.38903 to 3.34437, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 3.3444 - accuracy: 0.1206 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.2934 - accuracy: 0.1209\n",
      "Epoch 62: loss improved from 3.34437 to 3.29342, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 3.2934 - accuracy: 0.1209 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 3.2575 - accuracy: 0.1333\n",
      "Epoch 63: loss improved from 3.29342 to 3.26039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 3.2604 - accuracy: 0.1327 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.2249 - accuracy: 0.1332\n",
      "Epoch 64: loss improved from 3.26039 to 3.22490, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 3.2249 - accuracy: 0.1332 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.1901 - accuracy: 0.1430\n",
      "Epoch 65: loss improved from 3.22490 to 3.19011, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 3.1901 - accuracy: 0.1430 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 3.1441 - accuracy: 0.1506\n",
      "Epoch 66: loss improved from 3.19011 to 3.15249, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 3.1525 - accuracy: 0.1494 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 3.0987 - accuracy: 0.1544\n",
      "Epoch 67: loss improved from 3.15249 to 3.10692, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 3.1069 - accuracy: 0.1538 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0738 - accuracy: 0.1635\n",
      "Epoch 68: loss improved from 3.10692 to 3.07378, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 3.0738 - accuracy: 0.1635 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0295 - accuracy: 0.1687\n",
      "Epoch 69: loss improved from 3.07378 to 3.02948, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 3.0295 - accuracy: 0.1687 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.9888 - accuracy: 0.1723\n",
      "Epoch 70: loss improved from 3.02948 to 2.98880, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 2.9888 - accuracy: 0.1723 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.9385 - accuracy: 0.1787\n",
      "Epoch 71: loss improved from 2.98880 to 2.93846, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 2.9385 - accuracy: 0.1787 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.9111 - accuracy: 0.1826\n",
      "Epoch 72: loss improved from 2.93846 to 2.91417, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 2.9142 - accuracy: 0.1826 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8740 - accuracy: 0.1908\n",
      "Epoch 73: loss improved from 2.91417 to 2.87403, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 2.8740 - accuracy: 0.1908 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.8379 - accuracy: 0.1919\n",
      "Epoch 74: loss improved from 2.87403 to 2.84350, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.8435 - accuracy: 0.1916 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.8133 - accuracy: 0.1930\n",
      "Epoch 75: loss improved from 2.84350 to 2.81719, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.8172 - accuracy: 0.1929 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 2.7618 - accuracy: 0.2095\n",
      "Epoch 76: loss improved from 2.81719 to 2.76488, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 2.7649 - accuracy: 0.2096 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 2.6997 - accuracy: 0.2140\n",
      "Epoch 77: loss improved from 2.76488 to 2.71134, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 2.7113 - accuracy: 0.2103 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 2.6440 - accuracy: 0.2355\n",
      "Epoch 78: loss improved from 2.71134 to 2.66038, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 2.6604 - accuracy: 0.2312 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.6243 - accuracy: 0.2352\n",
      "Epoch 79: loss improved from 2.66038 to 2.62826, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.6283 - accuracy: 0.2343 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5869 - accuracy: 0.2445\n",
      "Epoch 80: loss improved from 2.62826 to 2.58694, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.5869 - accuracy: 0.2445 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.5472 - accuracy: 0.2586\n",
      "Epoch 81: loss improved from 2.58694 to 2.54790, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.5479 - accuracy: 0.2582 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 2.5047 - accuracy: 0.2601\n",
      "Epoch 82: loss improved from 2.54790 to 2.51333, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.5133 - accuracy: 0.2582 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.4786 - accuracy: 0.2594\n",
      "Epoch 83: loss improved from 2.51333 to 2.48304, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.4830 - accuracy: 0.2582 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 2.4407 - accuracy: 0.2821\n",
      "Epoch 84: loss improved from 2.48304 to 2.44801, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.4480 - accuracy: 0.2774 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.3966 - accuracy: 0.2836\n",
      "Epoch 85: loss improved from 2.44801 to 2.40160, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.4016 - accuracy: 0.2828 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.3621 - accuracy: 0.2820\n",
      "Epoch 86: loss improved from 2.40160 to 2.36810, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 2.3681 - accuracy: 0.2810 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.3183 - accuracy: 0.3042\n",
      "Epoch 87: loss improved from 2.36810 to 2.32178, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 29ms/step - loss: 2.3218 - accuracy: 0.3037 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2895 - accuracy: 0.2949\n",
      "Epoch 88: loss improved from 2.32178 to 2.28954, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 2.2895 - accuracy: 0.2949 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2591 - accuracy: 0.3086\n",
      "Epoch 89: loss improved from 2.28954 to 2.25912, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 2.2591 - accuracy: 0.3086 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 2.2294 - accuracy: 0.3154\n",
      "Epoch 90: loss improved from 2.25912 to 2.23525, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 2.2352 - accuracy: 0.3122 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1927 - accuracy: 0.3224\n",
      "Epoch 91: loss improved from 2.23525 to 2.19272, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 2.1927 - accuracy: 0.3224 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.1406 - accuracy: 0.3320\n",
      "Epoch 92: loss improved from 2.19272 to 2.14297, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.1430 - accuracy: 0.3312 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1031 - accuracy: 0.3466\n",
      "Epoch 93: loss improved from 2.14297 to 2.10311, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.1031 - accuracy: 0.3466 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0719 - accuracy: 0.3492\n",
      "Epoch 94: loss improved from 2.10311 to 2.07187, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.0719 - accuracy: 0.3492 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 2.0333 - accuracy: 0.3555\n",
      "Epoch 95: loss improved from 2.07187 to 2.03718, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.0372 - accuracy: 0.3548 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0027 - accuracy: 0.3561\n",
      "Epoch 96: loss improved from 2.03718 to 2.00269, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 2.0027 - accuracy: 0.3561 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.9663 - accuracy: 0.3672\n",
      "Epoch 97: loss improved from 2.00269 to 1.96582, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.9658 - accuracy: 0.3664 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.9289 - accuracy: 0.3745\n",
      "Epoch 98: loss improved from 1.96582 to 1.93862, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.9386 - accuracy: 0.3713 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9035 - accuracy: 0.3780\n",
      "Epoch 99: loss improved from 1.93862 to 1.90354, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.9035 - accuracy: 0.3780 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.8798 - accuracy: 0.3833\n",
      "Epoch 100: loss improved from 1.90354 to 1.87966, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 28ms/step - loss: 1.8797 - accuracy: 0.3839 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.8477 - accuracy: 0.4026\n",
      "Epoch 101: loss improved from 1.87966 to 1.84966, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 1.8497 - accuracy: 0.4011 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.8013 - accuracy: 0.4025\n",
      "Epoch 102: loss improved from 1.84966 to 1.80860, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 1.8086 - accuracy: 0.3986 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 1.7741 - accuracy: 0.4149\n",
      "Epoch 103: loss improved from 1.80860 to 1.78157, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.7816 - accuracy: 0.4114 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7392 - accuracy: 0.4199\n",
      "Epoch 104: loss improved from 1.78157 to 1.73920, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.7392 - accuracy: 0.4199 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 1.7021 - accuracy: 0.4232\n",
      "Epoch 105: loss improved from 1.73920 to 1.70807, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.7081 - accuracy: 0.4222 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.6784 - accuracy: 0.4303\n",
      "Epoch 106: loss improved from 1.70807 to 1.68131, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.6813 - accuracy: 0.4294 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.6491 - accuracy: 0.4354\n",
      "Epoch 107: loss improved from 1.68131 to 1.65032, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.6503 - accuracy: 0.4353 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6283 - accuracy: 0.4490\n",
      "Epoch 108: loss improved from 1.65032 to 1.62829, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.6283 - accuracy: 0.4490 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.5838 - accuracy: 0.4555\n",
      "Epoch 109: loss improved from 1.62829 to 1.59703, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.5970 - accuracy: 0.4502 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5741 - accuracy: 0.4497\n",
      "Epoch 110: loss improved from 1.59703 to 1.57406, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.5741 - accuracy: 0.4497 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.4661\n",
      "Epoch 111: loss improved from 1.57406 to 1.53350, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.5335 - accuracy: 0.4646 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.5051 - accuracy: 0.4659\n",
      "Epoch 112: loss improved from 1.53350 to 1.51209, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.5121 - accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5015 - accuracy: 0.4747\n",
      "Epoch 113: loss improved from 1.51209 to 1.50146, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 1.5015 - accuracy: 0.4747 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.4714 - accuracy: 0.4730\n",
      "Epoch 114: loss improved from 1.50146 to 1.47619, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 1.4762 - accuracy: 0.4711 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4572 - accuracy: 0.4739\n",
      "Epoch 115: loss improved from 1.47619 to 1.45717, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 1.4572 - accuracy: 0.4739 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.4372 - accuracy: 0.4812\n",
      "Epoch 116: loss improved from 1.45717 to 1.44068, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.4407 - accuracy: 0.4803 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.4105 - accuracy: 0.4865\n",
      "Epoch 117: loss improved from 1.44068 to 1.41458, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.4146 - accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 1.3852 - accuracy: 0.5011\n",
      "Epoch 118: loss improved from 1.41458 to 1.40111, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.4011 - accuracy: 0.4945 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.3745 - accuracy: 0.4899\n",
      "Epoch 119: loss improved from 1.40111 to 1.38023, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.3802 - accuracy: 0.4883 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.3484 - accuracy: 0.5064\n",
      "Epoch 120: loss improved from 1.38023 to 1.35520, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.3552 - accuracy: 0.5027 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3387 - accuracy: 0.5040\n",
      "Epoch 121: loss improved from 1.35520 to 1.33873, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.3387 - accuracy: 0.5040 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3269 - accuracy: 0.5145\n",
      "Epoch 122: loss improved from 1.33873 to 1.32686, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.3269 - accuracy: 0.5145 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.3225 - accuracy: 0.5065\n",
      "Epoch 123: loss improved from 1.32686 to 1.32505, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.3250 - accuracy: 0.5058 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2961 - accuracy: 0.5174\n",
      "Epoch 124: loss improved from 1.32505 to 1.29614, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2961 - accuracy: 0.5174 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2897 - accuracy: 0.5125\n",
      "Epoch 125: loss improved from 1.29614 to 1.28967, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2897 - accuracy: 0.5125 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2758 - accuracy: 0.5256\n",
      "Epoch 126: loss improved from 1.28967 to 1.27577, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2758 - accuracy: 0.5256 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2499 - accuracy: 0.5251\n",
      "Epoch 127: loss improved from 1.27577 to 1.24993, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2499 - accuracy: 0.5251 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2355 - accuracy: 0.5266\n",
      "Epoch 128: loss improved from 1.24993 to 1.23554, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2355 - accuracy: 0.5266 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2236 - accuracy: 0.5251\n",
      "Epoch 129: loss improved from 1.23554 to 1.22359, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2236 - accuracy: 0.5251 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2112 - accuracy: 0.5369\n",
      "Epoch 130: loss improved from 1.22359 to 1.21118, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.2112 - accuracy: 0.5369 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.1952 - accuracy: 0.5326\n",
      "Epoch 131: loss improved from 1.21118 to 1.19714, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.1971 - accuracy: 0.5323 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1821 - accuracy: 0.5405\n",
      "Epoch 132: loss improved from 1.19714 to 1.18206, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 1.1821 - accuracy: 0.5405 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.1635 - accuracy: 0.5437\n",
      "Epoch 133: loss improved from 1.18206 to 1.16359, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 1.1636 - accuracy: 0.5446 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.1464 - accuracy: 0.5427\n",
      "Epoch 134: loss improved from 1.16359 to 1.15062, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.1506 - accuracy: 0.5413 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.1312 - accuracy: 0.5557\n",
      "Epoch 135: loss improved from 1.15062 to 1.13476, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 1.1348 - accuracy: 0.5546 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.1157 - accuracy: 0.5559\n",
      "Epoch 136: loss improved from 1.13476 to 1.12145, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.1214 - accuracy: 0.5531 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1126 - accuracy: 0.5582\n",
      "Epoch 137: loss improved from 1.12145 to 1.11255, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.1126 - accuracy: 0.5582 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0980 - accuracy: 0.5536\n",
      "Epoch 138: loss improved from 1.11255 to 1.09798, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0980 - accuracy: 0.5536 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.5608\n",
      "Epoch 139: loss improved from 1.09798 to 1.08864, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0886 - accuracy: 0.5608 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0768 - accuracy: 0.5659\n",
      "Epoch 140: loss improved from 1.08864 to 1.08293, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0829 - accuracy: 0.5639 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.0685 - accuracy: 0.5651\n",
      "Epoch 141: loss improved from 1.08293 to 1.07371, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0737 - accuracy: 0.5624 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0570 - accuracy: 0.5606\n",
      "Epoch 142: loss improved from 1.07371 to 1.05701, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0570 - accuracy: 0.5606 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0463 - accuracy: 0.5685\n",
      "Epoch 143: loss improved from 1.05701 to 1.04761, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0476 - accuracy: 0.5678 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0379 - accuracy: 0.5629\n",
      "Epoch 144: loss improved from 1.04761 to 1.03791, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0379 - accuracy: 0.5629 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0311 - accuracy: 0.5674\n",
      "Epoch 145: loss improved from 1.03791 to 1.03471, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.0347 - accuracy: 0.5652 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0149 - accuracy: 0.5753\n",
      "Epoch 146: loss improved from 1.03471 to 1.02129, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 1.0213 - accuracy: 0.5719 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.5731\n",
      "Epoch 147: loss did not improve from 1.02129\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 1.0221 - accuracy: 0.5708 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.0119 - accuracy: 0.5699\n",
      "Epoch 148: loss improved from 1.02129 to 1.01689, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.0169 - accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 1.0110 - accuracy: 0.5695\n",
      "Epoch 149: loss did not improve from 1.01689\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.0172 - accuracy: 0.5670 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.9880 - accuracy: 0.5714\n",
      "Epoch 150: loss improved from 1.01689 to 0.99458, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.9946 - accuracy: 0.5690 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.9888 - accuracy: 0.5729\n",
      "Epoch 151: loss improved from 0.99458 to 0.99201, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.9920 - accuracy: 0.5721 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9861 - accuracy: 0.5721\n",
      "Epoch 152: loss improved from 0.99201 to 0.98614, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.9861 - accuracy: 0.5721 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.9790 - accuracy: 0.5742\n",
      "Epoch 153: loss improved from 0.98614 to 0.98372, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.9837 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9981 - accuracy: 0.5631\n",
      "Epoch 154: loss did not improve from 0.98372\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.9981 - accuracy: 0.5631 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.9838 - accuracy: 0.5703\n",
      "Epoch 155: loss did not improve from 0.98372\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.9855 - accuracy: 0.5701 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.9952 - accuracy: 0.5680\n",
      "Epoch 156: loss did not improve from 0.98372\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.9961 - accuracy: 0.5678 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7872 - accuracy: 0.6318\n",
      "Epoch 157: loss improved from 0.98372 to 0.78724, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7872 - accuracy: 0.6318 - lr: 2.0000e-04\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.6238\n",
      "Epoch 158: loss improved from 0.78724 to 0.73922, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7392 - accuracy: 0.6238 - lr: 2.0000e-04\n",
      "Epoch 159/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7218 - accuracy: 0.6154\n",
      "Epoch 159: loss improved from 0.73922 to 0.72401, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7240 - accuracy: 0.6122 - lr: 2.0000e-04\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7171 - accuracy: 0.6040\n",
      "Epoch 160: loss improved from 0.72401 to 0.71711, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7171 - accuracy: 0.6040 - lr: 2.0000e-04\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.6017\n",
      "Epoch 161: loss improved from 0.71711 to 0.71459, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7146 - accuracy: 0.6017 - lr: 2.0000e-04\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.5904\n",
      "Epoch 162: loss improved from 0.71459 to 0.71092, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7109 - accuracy: 0.5904 - lr: 2.0000e-04\n",
      "Epoch 163/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7077 - accuracy: 0.5898\n",
      "Epoch 163: loss improved from 0.71092 to 0.70991, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.7099 - accuracy: 0.5876 - lr: 2.0000e-04\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.5804\n",
      "Epoch 164: loss improved from 0.70991 to 0.70783, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7078 - accuracy: 0.5804 - lr: 2.0000e-04\n",
      "Epoch 165/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.7040 - accuracy: 0.5821\n",
      "Epoch 165: loss improved from 0.70783 to 0.70768, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.7077 - accuracy: 0.5801 - lr: 2.0000e-04\n",
      "Epoch 166/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7039 - accuracy: 0.5870\n",
      "Epoch 166: loss improved from 0.70768 to 0.70530, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 36ms/step - loss: 0.7053 - accuracy: 0.5858 - lr: 2.0000e-04\n",
      "Epoch 167/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7059 - accuracy: 0.5807\n",
      "Epoch 167: loss improved from 0.70530 to 0.70489, saving model to nextword1.h5\n",
      "61/61 [==============================] - 3s 43ms/step - loss: 0.7049 - accuracy: 0.5811 - lr: 2.0000e-04\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7046 - accuracy: 0.5819\n",
      "Epoch 168: loss improved from 0.70489 to 0.70464, saving model to nextword1.h5\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 0.7046 - accuracy: 0.5819 - lr: 2.0000e-04\n",
      "Epoch 169/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7021 - accuracy: 0.5836\n",
      "Epoch 169: loss improved from 0.70464 to 0.70199, saving model to nextword1.h5\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.7020 - accuracy: 0.5829 - lr: 2.0000e-04\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.5811\n",
      "Epoch 170: loss improved from 0.70199 to 0.70141, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.7014 - accuracy: 0.5811 - lr: 2.0000e-04\n",
      "Epoch 171/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.7006 - accuracy: 0.5871\n",
      "Epoch 171: loss did not improve from 0.70141\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7022 - accuracy: 0.5834 - lr: 2.0000e-04\n",
      "Epoch 172/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.6990 - accuracy: 0.5863\n",
      "Epoch 172: loss improved from 0.70141 to 0.70121, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7012 - accuracy: 0.5837 - lr: 2.0000e-04\n",
      "Epoch 173/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.7022 - accuracy: 0.5826\n",
      "Epoch 173: loss did not improve from 0.70121\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.7015 - accuracy: 0.5819 - lr: 2.0000e-04\n",
      "Epoch 174/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.7001 - accuracy: 0.5810\n",
      "Epoch 174: loss improved from 0.70121 to 0.69949, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6995 - accuracy: 0.5814 - lr: 2.0000e-04\n",
      "Epoch 175/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.6953 - accuracy: 0.5885\n",
      "Epoch 175: loss improved from 0.69949 to 0.69852, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6985 - accuracy: 0.5850 - lr: 2.0000e-04\n",
      "Epoch 176/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5823\n",
      "Epoch 176: loss did not improve from 0.69852\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6987 - accuracy: 0.5816 - lr: 2.0000e-04\n",
      "Epoch 177/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6947 - accuracy: 0.5854\n",
      "Epoch 177: loss improved from 0.69852 to 0.69773, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6977 - accuracy: 0.5834 - lr: 2.0000e-04\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.5791\n",
      "Epoch 178: loss improved from 0.69773 to 0.69759, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6976 - accuracy: 0.5791 - lr: 2.0000e-04\n",
      "Epoch 179/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.5832\n",
      "Epoch 179: loss improved from 0.69759 to 0.69651, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6965 - accuracy: 0.5809 - lr: 2.0000e-04\n",
      "Epoch 180/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6954 - accuracy: 0.5781\n",
      "Epoch 180: loss improved from 0.69651 to 0.69483, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6948 - accuracy: 0.5786 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.5801\n",
      "Epoch 181: loss improved from 0.69483 to 0.69475, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6947 - accuracy: 0.5801 - lr: 2.0000e-04\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5796\n",
      "Epoch 182: loss improved from 0.69475 to 0.69428, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6943 - accuracy: 0.5796 - lr: 2.0000e-04\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5780\n",
      "Epoch 183: loss improved from 0.69428 to 0.69387, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6939 - accuracy: 0.5780 - lr: 2.0000e-04\n",
      "Epoch 184/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.5841\n",
      "Epoch 184: loss did not improve from 0.69387\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.6941 - accuracy: 0.5829 - lr: 2.0000e-04\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5827\n",
      "Epoch 185: loss improved from 0.69387 to 0.69310, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6931 - accuracy: 0.5827 - lr: 2.0000e-04\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.5816\n",
      "Epoch 186: loss improved from 0.69310 to 0.69161, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6916 - accuracy: 0.5816 - lr: 2.0000e-04\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5837\n",
      "Epoch 187: loss did not improve from 0.69161\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.6919 - accuracy: 0.5837 - lr: 2.0000e-04\n",
      "Epoch 188/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.5805\n",
      "Epoch 188: loss improved from 0.69161 to 0.69101, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6910 - accuracy: 0.5796 - lr: 2.0000e-04\n",
      "Epoch 189/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.5849\n",
      "Epoch 189: loss improved from 0.69101 to 0.69078, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6908 - accuracy: 0.5811 - lr: 2.0000e-04\n",
      "Epoch 190/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5776\n",
      "Epoch 190: loss improved from 0.69078 to 0.68989, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6899 - accuracy: 0.5780 - lr: 2.0000e-04\n",
      "Epoch 191/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.6909 - accuracy: 0.5835\n",
      "Epoch 191: loss did not improve from 0.68989\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 0.6900 - accuracy: 0.5819 - lr: 2.0000e-04\n",
      "Epoch 192/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5794\n",
      "Epoch 192: loss improved from 0.68989 to 0.68924, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6892 - accuracy: 0.5770 - lr: 2.0000e-04\n",
      "Epoch 193/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5740\n",
      "Epoch 193: loss improved from 0.68924 to 0.68875, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6888 - accuracy: 0.5739 - lr: 2.0000e-04\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.5726\n",
      "Epoch 194: loss improved from 0.68875 to 0.68735, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6873 - accuracy: 0.5726 - lr: 2.0000e-04\n",
      "Epoch 195/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5813\n",
      "Epoch 195: loss improved from 0.68735 to 0.68731, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6873 - accuracy: 0.5783 - lr: 2.0000e-04\n",
      "Epoch 196/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5771\n",
      "Epoch 196: loss improved from 0.68731 to 0.68668, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6867 - accuracy: 0.5747 - lr: 2.0000e-04\n",
      "Epoch 197/200\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5807\n",
      "Epoch 197: loss improved from 0.68668 to 0.68585, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 0.6859 - accuracy: 0.5780 - lr: 2.0000e-04\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6847 - accuracy: 0.5837\n",
      "Epoch 198: loss improved from 0.68585 to 0.68469, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.6847 - accuracy: 0.5837 - lr: 2.0000e-04\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.5773\n",
      "Epoch 199: loss did not improve from 0.68469\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.6857 - accuracy: 0.5773 - lr: 2.0000e-04\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.5814\n",
      "Epoch 200: loss improved from 0.68469 to 0.68342, saving model to nextword1.h5\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.6834 - accuracy: 0.5814 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X, y, epochs=200, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69495d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021b6cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c819d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d0f27",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf7ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb026cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ...,   0,  53, 293])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(X) for x in sequences])\n",
    "input_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b296e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: In the morning I\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 3888).\n",
      "Suggested next two word are :  ['bearing', 'visible']\n",
      "In the morning I bearing visible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while(True):\n",
    "    seed_text = input(\"Enter your line: \")\n",
    "  \n",
    "    if seed_text == \"0\":\n",
    "        print(\"Execution completed.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            next_words = 2\n",
    "            suggested_word = []\n",
    "            #temp = seed_text\n",
    "            for _ in range(next_words):\n",
    "                \n",
    "                token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "                #print(token_list)\n",
    "                token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "                predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "                output_word = \"\"\n",
    "                \n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == predicted:\n",
    "                        output_word = word\n",
    "                        suggested_word.append(output_word)\n",
    "                        break\n",
    "                \n",
    "                seed_text += \" \" + output_word\n",
    "            print(\"Suggested next two word are : \",suggested_word)\n",
    "            #n = int(input())\n",
    "            \n",
    "            print(seed_text)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23e3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
